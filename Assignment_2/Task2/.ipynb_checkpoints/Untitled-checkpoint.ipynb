{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "animals = ['cavallo', 'pecora', 'gatto', 'farfalla', 'cane']\n",
    "datasetDir = \"Dataset/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generateDatasetCSV():\n",
    "    newlines = []\n",
    "    for i,animal in enumerate(animals):\n",
    "        \n",
    "        with open(f\"{datasetDir+animal}.csv\",\"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                newlines.append(','.join(line.strip().split(\",\")[1:49])+f\",{i}\")\n",
    "    \n",
    "    random.shuffle(newlines)\n",
    "    train_lines = newlines[0:int(0.8*len(newlines))]\n",
    "    test_lines = newlines[int(0.8*len(newlines)):len(newlines)]\n",
    "    \n",
    "    with open(f\"{datasetDir}train.csv\",\"w\") as f:\n",
    "        for line in train_lines:\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "    with open(f\"{datasetDir}test.csv\",\"w\") as f:\n",
    "        for line in test_lines:\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    \n",
    "            \n",
    "  \n",
    "            \n",
    "    \n",
    "\n",
    "        \n",
    "                \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "generateDatasetCSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self,data_source):\n",
    "        with open(data_source,\"r\") as f:\n",
    "            self.lines = f.readlines()\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        for line in self.lines[0:]:            \n",
    "            self.linedata = tuple(map(float,line.strip().split(\",\")))\n",
    "            self.inputs.append(torch.tensor(self.linedata[0:48])/189000.0)\n",
    "            self.outputs.append(torch.tensor(self.linedata[48]))\n",
    "    def __len__(self):\n",
    "        return len(self.outputs)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.inputs[idx],self.outputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AANN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_features=48,out_features=64),nn.ReLU(),nn.Linear(in_features=64,out_features=24))\n",
    "        self.decoder = nn.Sequential(nn.Linear(in_features=24,out_features=64),nn.ReLU(),nn.Linear(in_features=64,out_features=48))\n",
    "    def forward(self,x,path=\"all\"):\n",
    "        if(path == \"all\"):\n",
    "            return self.decoder(self.encoder(x))\n",
    "        else:\n",
    "            return self.encoder(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AANN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_features=24,out_features=32),nn.ReLU(),nn.Linear(in_features=32,out_features=12))\n",
    "        self.decoder = nn.Sequential(nn.Linear(in_features=12,out_features=32),nn.ReLU(),nn.Linear(in_features=32,out_features=24))\n",
    "    def forward(self,x,path=\"all\"):\n",
    "        if(path == \"all\"):\n",
    "            return self.decoder(self.encoder(x))\n",
    "        else:\n",
    "            return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AANN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_features=12,out_features=18),nn.ReLU(),nn.Linear(in_features=18,out_features=6))\n",
    "        self.decoder = nn.Sequential(nn.Linear(in_features=6,out_features=18),nn.ReLU(),nn.Linear(in_features=18,out_features=12))\n",
    "    def forward(self,x,path=\"all\"):\n",
    "        if(path == \"all\"):\n",
    "            return self.decoder(self.encoder(x))\n",
    "        else:\n",
    "            return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataset = AnimalDataset(data_source=f\"{datasetDir}train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "trainLoader  = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "aann1 = AANN1().to(device)\n",
    "aann2 = AANN2().to(device)\n",
    "aann3 = AANN3().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:15: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  from ipykernel import kernelapp as app\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 average loss: 0.026868975674622382\n",
      "Epoch 1 average loss: 0.026528769415005295\n",
      "Epoch 2 average loss: 0.02620071374982382\n",
      "Epoch 3 average loss: 0.025947065494936805\n",
      "Epoch 4 average loss: 0.025706292163393946\n",
      "Epoch 5 average loss: 0.025400824235810016\n",
      "Epoch 6 average loss: 0.025161735926863688\n",
      "Epoch 7 average loss: 0.024939708869575316\n",
      "Epoch 8 average loss: 0.024754085744728906\n",
      "Epoch 9 average loss: 0.024556502073043003\n",
      "Epoch 10 average loss: 0.02435596805368161\n",
      "Epoch 11 average loss: 0.024171101429493018\n",
      "Epoch 12 average loss: 0.02402013270206625\n",
      "Epoch 13 average loss: 0.023846345149016183\n",
      "Epoch 14 average loss: 0.02370074089518737\n",
      "Epoch 15 average loss: 0.023496692573433845\n",
      "Epoch 16 average loss: 0.023361312621106394\n",
      "Epoch 17 average loss: 0.023238196491146\n",
      "Epoch 18 average loss: 0.023127351551892366\n",
      "Epoch 19 average loss: 0.022952263592464543\n",
      "Epoch 20 average loss: 0.02280918116679055\n",
      "Epoch 21 average loss: 0.02264544482927476\n",
      "Epoch 22 average loss: 0.022494155329014207\n",
      "Epoch 23 average loss: 0.02239879395874986\n",
      "Epoch 24 average loss: 0.022255095135340543\n",
      "Epoch 25 average loss: 0.022086333310084867\n",
      "Epoch 26 average loss: 0.02192005592004923\n",
      "Epoch 27 average loss: 0.02177100675069418\n",
      "Epoch 28 average loss: 0.021633552958597573\n",
      "Epoch 29 average loss: 0.021491474185254524\n",
      "Epoch 30 average loss: 0.02137236534740091\n",
      "Epoch 31 average loss: 0.021216286528579945\n",
      "Epoch 32 average loss: 0.02103077470678347\n",
      "Epoch 33 average loss: 0.02088271521102984\n",
      "Epoch 34 average loss: 0.02077178300529505\n",
      "Epoch 35 average loss: 0.020629955637924427\n",
      "Epoch 36 average loss: 0.02048497013569874\n",
      "Epoch 37 average loss: 0.020394971769809866\n",
      "Epoch 38 average loss: 0.020281797537228943\n",
      "Epoch 39 average loss: 0.02014483204933866\n",
      "Epoch 40 average loss: 0.020017113834441703\n",
      "Epoch 41 average loss: 0.019885807647091962\n",
      "Epoch 42 average loss: 0.01981142167722126\n",
      "Epoch 43 average loss: 0.019651406844700008\n",
      "Epoch 44 average loss: 0.01958379260573296\n",
      "Epoch 45 average loss: 0.01943892110939313\n",
      "Epoch 46 average loss: 0.01933999359829969\n",
      "Epoch 47 average loss: 0.019254882586564256\n",
      "Epoch 48 average loss: 0.01912623803689187\n",
      "Epoch 49 average loss: 0.01904562899625031\n",
      "Epoch 50 average loss: 0.018909214689065135\n",
      "Epoch 51 average loss: 0.018824791775789638\n",
      "Epoch 52 average loss: 0.018735355686458597\n",
      "Epoch 53 average loss: 0.01866628263625794\n",
      "Epoch 54 average loss: 0.018596516604266332\n",
      "Epoch 55 average loss: 0.01851867560768469\n",
      "Epoch 56 average loss: 0.018441569735796186\n",
      "Epoch 57 average loss: 0.0183922807888299\n",
      "Epoch 58 average loss: 0.0182993237867169\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-72-a45fa62beaa2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     19\u001b[0m         \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 21\u001b[1;33m         \u001b[0maann1_optimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     22\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     23\u001b[0m     \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Epoch {epoch} average loss: {total_loss/cnt}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\autograd\\grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     24\u001b[0m         \u001b[1;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     25\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 26\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     27\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mF\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\optim\\adam.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, closure)\u001b[0m\n\u001b[0;32m    117\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'lr'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    118\u001b[0m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'weight_decay'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 119\u001b[1;33m                    \u001b[0mgroup\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'eps'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    120\u001b[0m                    )\n\u001b[0;32m    121\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\python37\\lib\\site-packages\\torch\\optim\\functional.py\u001b[0m in \u001b[0;36madam\u001b[1;34m(params, grads, exp_avgs, exp_avg_sqs, max_exp_avg_sqs, state_steps, amsgrad, beta1, beta2, lr, weight_decay, eps)\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mmax_exp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     93\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 94\u001b[1;33m             \u001b[0mdenom\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mexp_avg_sq\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbias_correction2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0meps\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     95\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     96\u001b[0m         \u001b[0mstep_size\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlr\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mbias_correction1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Training AANN1\n",
    "\n",
    "aann1_optimizer = optim.Adam(aann1.parameters(),lr=1e-3)\n",
    "criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    for i,(data,target) in enumerate(trainLoader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = aann1(data)\n",
    "        \n",
    "        loss = criterion(nn.functional.log_softmax(out),data)\n",
    "        cnt+=1\n",
    "        total_loss += loss.item() \n",
    "        aann1_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        aann1_optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch} average loss: {total_loss/cnt}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python37\\lib\\site-packages\\ipykernel_launcher.py:1: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([ 5090.3906,  9189.8975, 11022.6904, 10167.5215,  6589.3301,  3613.2178,\n",
       "         2116.2864,  1428.9631,  1051.1146,   745.5815,   439.1728,   275.9613,\n",
       "          211.6774,   289.3924,   648.0627,  8267.7334,   868.9223,  1650.4288,\n",
       "         1980.0326,  2445.3462,  3140.3135,  5564.1543,  9178.0664, 12634.3721,\n",
       "         9722.0654,  4583.9521,  1646.0823,   706.9911,   412.4859,   409.3842,\n",
       "          847.1410,  7964.3823,   531.7623,  1538.5402,  2644.5200,  4137.0640,\n",
       "         6755.7930,  9473.6289, 10517.5596,  8367.0215,  4867.1094,  2325.2578,\n",
       "         1148.3171,   716.1161,   657.7848,   927.7845,  1631.8126,  7858.8188],\n",
       "       device='cuda:0', grad_fn=<MulBackward0>)"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.functional.softmax(aann1(trainDataset[0][0].to(device)))*189000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 4609.0000,  9129.0000, 12030.0000, 10840.0000,  5547.0000,  3127.0000,\n",
       "         2927.0000,  2348.0000,  1321.0000,   735.0000,   413.0000,   238.0000,\n",
       "          108.0000,   104.0000,   189.0000,  9335.0000,  1090.0000,  1513.0000,\n",
       "         1950.0000,  2477.0000,  4344.0000,  6675.0005,  8671.0000, 10078.0000,\n",
       "         8745.0000,  4796.0000,  2227.0000,   626.0000,   149.0000,   105.0000,\n",
       "          867.9999,  8686.0000,   731.0000,  1417.0000,  2396.0000,  4185.0000,\n",
       "         7035.9995,  9574.0000, 10475.0000,  7965.0000,  5578.0000,  2620.0000,\n",
       "          930.0000,   344.0000,   177.0000,   292.0000,  4079.0000,  5201.0000])"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainDataset[0][0]*189000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
