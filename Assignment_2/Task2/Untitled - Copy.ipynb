{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Task 2: Stacked Autoencoder based Pre-Training of DNN and Fine Tuning</h1></center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0iobt83Sg1HB"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wxrbmh0Gg1HO"
   },
   "outputs": [],
   "source": [
    "animals = ['cavallo', 'pecora', 'gatto', 'farfalla', 'cane']\n",
    "datasetDir = \"Dataset/\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Generating the Dataset</h2>\n",
    "The dataset is generated by combining all csvs, and randomly shuffling the data and then doing an 80-20 train-test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wToxjg47g1HQ"
   },
   "outputs": [],
   "source": [
    "def generateDatasetCSV():\n",
    "    newlines = []\n",
    "    for i,animal in enumerate(animals):\n",
    "        \n",
    "        with open(f\"{datasetDir+animal}.csv\",\"r\") as f:\n",
    "            lines = f.readlines()\n",
    "            for line in lines:\n",
    "                newlines.append(','.join(line.strip().split(\",\")[1:49])+f\",{i}\")\n",
    "    \n",
    "    random.shuffle(newlines)\n",
    "    train_lines = newlines[0:int(0.8*len(newlines))]\n",
    "    test_lines = newlines[int(0.8*len(newlines)):len(newlines)]\n",
    "    \n",
    "    with open(f\"{datasetDir}train.csv\",\"w\") as f:\n",
    "        for line in train_lines:\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "    with open(f\"{datasetDir}test.csv\",\"w\") as f:\n",
    "        for line in test_lines:\n",
    "            f.write(line)\n",
    "            f.write(\"\\n\")\n",
    "            \n",
    "    \n",
    "            \n",
    "  \n",
    "            \n",
    "    \n",
    "\n",
    "        \n",
    "                \n",
    "        \n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fgy7PAoQg1HS"
   },
   "outputs": [],
   "source": [
    "generateDatasetCSV()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jHPW0Cgsg1HU"
   },
   "outputs": [],
   "source": [
    "class AnimalDataset(Dataset):\n",
    "    def __init__(self,data_source):\n",
    "        with open(data_source,\"r\") as f:\n",
    "            self.lines = f.readlines()\n",
    "        self.inputs = []\n",
    "        self.outputs = []\n",
    "        for line in self.lines[0:]:            \n",
    "            self.linedata = tuple(map(float,line.strip().split(\",\")))\n",
    "            self.inputs.append(torch.tensor(self.linedata[0:48])/189000.0)\n",
    "            self.outputs.append(torch.tensor(self.linedata[48]))\n",
    "    def __len__(self):\n",
    "        return len(self.outputs)\n",
    "    def __getitem__(self,idx):\n",
    "        return self.inputs[idx],self.outputs[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dLarpQVFg1HW"
   },
   "outputs": [],
   "source": [
    "class AANN1(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_features=48,out_features=64),nn.Tanh(),nn.Linear(in_features=64,out_features=24))\n",
    "        self.decoder = nn.Sequential(nn.Linear(in_features=24,out_features=64),nn.Tanh(),nn.Linear(in_features=64,out_features=48))\n",
    "    def forward(self,x,path=\"all\"):\n",
    "        if(path == \"all\"):\n",
    "            return self.decoder(self.encoder(x))\n",
    "        else:\n",
    "            return self.encoder(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "z_YLTFptg1HX"
   },
   "outputs": [],
   "source": [
    "class AANN2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_features=24,out_features=32),nn.Tanh(),nn.Linear(in_features=32,out_features=12))\n",
    "        self.decoder = nn.Sequential(nn.Linear(in_features=12,out_features=32),nn.Tanh(),nn.Linear(in_features=32,out_features=24))\n",
    "    def forward(self,x,path=\"all\"):\n",
    "        if(path == \"all\"):\n",
    "            return self.decoder(self.encoder(x))\n",
    "        else:\n",
    "            return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gkr_6i94g1Ha"
   },
   "outputs": [],
   "source": [
    "class AANN3(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(nn.Linear(in_features=12,out_features=18),nn.Tanh(),nn.Linear(in_features=18,out_features=6))\n",
    "        self.decoder = nn.Sequential(nn.Linear(in_features=6,out_features=18),nn.Tanh(),nn.Linear(in_features=18,out_features=12))\n",
    "    def forward(self,x,path=\"all\"):\n",
    "        if(path == \"all\"):\n",
    "            return self.decoder(self.encoder(x))\n",
    "        else:\n",
    "            return self.encoder(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ahbAa6k0g1Hc"
   },
   "outputs": [],
   "source": [
    "trainDataset = AnimalDataset(data_source=f\"{datasetDir}train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9m8tW0xPg1Hf"
   },
   "outputs": [],
   "source": [
    "batch_size = 25\n",
    "trainLoader  = torch.utils.data.DataLoader(trainDataset, batch_size=batch_size,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "f5UZ9lwXg1Hh",
    "outputId": "62499b51-4e15-4b41-9264-b099e3d8d736"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "aann1 = AANN1().to(device)\n",
    "aann2 = AANN2().to(device)\n",
    "aann3 = AANN3().to(device)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training AANN1</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "L9W-gpMkg1Hj",
    "outputId": "11ad2962-6214-4fd1-f144-6adc935695ba"
   },
   "outputs": [],
   "source": [
    "# Training AANN1\n",
    "\n",
    "aann1_optimizer = optim.Adam(aann1.parameters(),lr=4e-4)\n",
    "criterion = nn.KLDivLoss(reduction=\"batchmean\")\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    for i,(data,target) in enumerate(trainLoader):\n",
    "        data = data.to(device)\n",
    "        \n",
    "        out = aann1(data)\n",
    "        \n",
    "        loss = criterion(nn.functional.log_softmax(out),data)\n",
    "        cnt+=1\n",
    "        total_loss += loss.item() \n",
    "        aann1_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        aann1_optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch} average loss: {total_loss/cnt}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "i2qjnmQOg1Hl"
   },
   "outputs": [],
   "source": [
    "torch.save(aann1.state_dict(), \"aann1.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yve0Otlwg1Hm",
    "outputId": "fb633379-6026-40c8-840d-75edbbaf91c1"
   },
   "outputs": [],
   "source": [
    "trainDataset[0][0]*189000.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XQWQjc06g1Hn",
    "outputId": "fc3d3bbe-a0d6-4356-b880-30a366b6bdca"
   },
   "outputs": [],
   "source": [
    "torch.nn.functional.softmax(aann1(trainDataset[0][0].to(device)))*189000.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training AANN2</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YLmI_taDplUi",
    "outputId": "66870436-6ddc-4e85-e05d-eef55985ee06"
   },
   "outputs": [],
   "source": [
    "# Training AANN2\n",
    "\n",
    "aann2_optimizer = optim.Adam(aann2.parameters(),lr=4e-4)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    for i,(data,target) in enumerate(trainLoader):\n",
    "        data = aann1(data.to(device),path=\"encoder\")\n",
    "        \n",
    "        out = aann2(data)\n",
    "        \n",
    "        loss = criterion(out,data)\n",
    "        cnt+=1\n",
    "        total_loss += loss.item() \n",
    "        aann2_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        aann2_optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch} average loss: {total_loss/cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EKV10eFfqAam"
   },
   "outputs": [],
   "source": [
    "torch.save(aann2.state_dict(), \"aann2.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "zaT_rT6gwIEW",
    "outputId": "c38bc757-419c-4007-afd2-29128e18e6a1"
   },
   "outputs": [],
   "source": [
    "aann1(trainDataset[0][0].to(device),path=\"encoder\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vnXNk0GqwV92",
    "outputId": "2a78989a-7273-4425-d4c7-a588a17a6c39"
   },
   "outputs": [],
   "source": [
    "aann2(aann1(trainDataset[0][0].to(device),path=\"encoder\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2>Training AANN3</h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0XtScaGxwdjm",
    "outputId": "6187cbe8-7fcb-4e68-ee9b-79fc3f574a4c"
   },
   "outputs": [],
   "source": [
    "# Training AANN3\n",
    "\n",
    "aann3_optimizer = optim.Adam(aann3.parameters(),lr=4e-4)\n",
    "criterion = nn.MSELoss()\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    for i,(data,target) in enumerate(trainLoader):\n",
    "        data = aann2(aann1(data.to(device),path=\"encoder\"),path=\"encoder\")\n",
    "        \n",
    "        out = aann3(data)\n",
    "        \n",
    "        loss = criterion(out,data)\n",
    "        cnt+=1\n",
    "        total_loss += loss.item() \n",
    "        aann3_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "        aann3_optimizer.step()\n",
    "    \n",
    "    print(f\"Epoch {epoch} average loss: {total_loss/cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xaEtP-y6xLFL"
   },
   "outputs": [],
   "source": [
    "torch.save(aann3.state_dict(), \"aann3.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DPPFHBZL3EvE",
    "outputId": "a606c888-48a6-4709-a7a4-e83323647c00"
   },
   "source": [
    "<h2>Fine Tuning Stacked Autoencoder by adding an Output Softmax Layer</h2>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lnIeoG6Tlh1B"
   },
   "outputs": [],
   "source": [
    "class StackedAE(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.aann1 = AANN1()\n",
    "        self.aann1.load_state_dict(torch.load(\"aann1.pth\"))\n",
    "\n",
    "        self.aann2 = AANN2()\n",
    "        self.aann2.load_state_dict(torch.load(\"aann2.pth\"))\n",
    "\n",
    "        self.aann3 = AANN3()\n",
    "        self.aann3.load_state_dict(torch.load(\"aann3.pth\"))\n",
    "        \n",
    "        self.output = nn.Sequential(nn.Linear(in_features=6,out_features=5))\n",
    "    def forward(self,x):\n",
    "        \n",
    "        return self.output(self.aann3(self.aann2(self.aann1(x,path=\"encoder\"),path=\"encoder\"),path=\"encoder\"))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "253lvlu0lh1C",
    "outputId": "7b012e42-9ef7-4de5-967f-5ef602399909"
   },
   "outputs": [],
   "source": [
    "# Fine Tuning Stacked Autoencoder\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "sae = StackedAE().to(device)\n",
    "\n",
    "sae_optimizer = optim.Adam(sae.parameters(),lr=6e-3)\n",
    "sae_scheduler = optim.lr_scheduler.ReduceLROnPlateau(sae_optimizer,\"min\")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "epochs = 1000\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    cnt = 0\n",
    "    for i,(data,target) in enumerate(trainLoader):\n",
    "        input_ = sae(data.to(device))\n",
    "        \n",
    "        \n",
    "        \n",
    "        loss = criterion(input_,target.long().to(device))\n",
    "        cnt+=1\n",
    "        total_loss += loss.item() \n",
    "        sae_optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "       \n",
    "        sae_optimizer.step()\n",
    "    sae_scheduler.step(total_loss/cnt)\n",
    "    print(f\"Epoch {epoch} average loss: {total_loss/cnt}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ewjzIWdAlh1F",
    "outputId": "38761097-ef3c-4d4b-ccd7-4b2b7fabf345"
   },
   "outputs": [],
   "source": [
    "\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xzd82C1llh1G"
   },
   "outputs": [],
   "source": [
    "torch.save(sae.state_dict(), \"sae.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lLn4gecwRMjh"
   },
   "outputs": [],
   "source": [
    "sae = StackedAE()\n",
    "sae.load_state_dict(torch.load(\"sae.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testDataset = AnimalDataset(data_source=f\"{datasetDir}test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "device = 'cpu'\n",
    "y_actual_train = []\n",
    "y_pred_train = []\n",
    "\n",
    "y_actual_test = []\n",
    "y_pred_test = []\n",
    "\n",
    "\n",
    "cnt = 0\n",
    "tot = 0\n",
    "for j in range(len(trainDataset)):\n",
    "        y_pred_train.append(torch.argmax(nn.functional.softmax(sae(trainDataset[j][0].to(device)))).item())\n",
    "        y_actual_train.append(trainDataset[j][1].item())\n",
    "for j in range(len(testDataset)):        \n",
    "        y_pred_test.append(torch.argmax(nn.functional.softmax(sae(testDataset[j][0].to(device)))).item())\n",
    "        y_actual_test.append(testDataset[j][1].item())\n",
    "        \n",
    "y_actual_train = np.array(y_actual_train)\n",
    "y_pred_train = np.array(y_pred_train)\n",
    "\n",
    "y_actual_test = np.array(y_actual_test)\n",
    "y_pred_test = np.array(y_pred_test)\n",
    "\n",
    "for j in range(len(y_actual_train)):\n",
    "    tot+=1\n",
    "    if(y_actual_train[j] == y_pred_train[j]):\n",
    "        cnt+=1\n",
    "print(\"Train Set Accuracy: \", cnt/tot)\n",
    "\n",
    "tot = 0\n",
    "cnt = 0\n",
    "\n",
    "for j in range(len(y_actual_test)):\n",
    "    tot+=1\n",
    "    if(y_actual_test[j] == y_pred_test[j]):\n",
    "        cnt+=1\n",
    "    \n",
    "print(\"Test Set Accuracy: \", cnt/tot)\n",
    " \n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_actual_test, y_pred_test, labels=[0,1,2,3,4])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=animals)\n",
    "\n",
    "disp.plot()\n",
    "plt.title(\"Test Set Confusion Matrix\")\n",
    "plt.savefig(\"test_confusion.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_actual_train, y_pred_train, labels=[0,1,2,3,4])\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=animals)\n",
    "\n",
    "disp.plot()\n",
    "plt.title(\"Train Set Confusion Matrix\")\n",
    "plt.savefig(\"train_confusion.png\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "freqs = [0 for i in range(5)]\n",
    "for j in range(len(y_actual_train)):\n",
    "    freqs[int(y_actual_train[j].item())]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.xlabel(\"Class\")\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.hist(y_actual_train)\n",
    "plt.xticks([0,1,2,3,4])\n",
    "plt.title(\"Data Distribution in the Training Set\")\n",
    "plt.savefig(\"data.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Untitled.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
